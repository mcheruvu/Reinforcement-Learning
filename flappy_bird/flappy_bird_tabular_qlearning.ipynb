{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Tabular: Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "**Value function**\n",
    "It would be great to know how **good** a given state s is. Something to tell us: no matter the state we are in if we transition to state s your total reward will be x, word! If you start from s and follow policy π. That would spare us from revisiting same states over and over again. The value function does this for us. It depends on the state we are in s and the policy π your agent is following. It is given by:\n",
    "\n",
    "![title](images/value_pi.png)\n",
    "\n",
    "There exists an optimal value function that has the highest value for all states. It is given by:\n",
    "![title](images/v_max.png)\n",
    "\n",
    "**Q function**\n",
    "Agent can not control what state it ends up in, directly. It can influence enviornment by choosing some action **a**. Let us introduce another function that accepts state and action as parameters and returns the expected total reward — the Q function (it represents the **quality** of a certain action given a state). More formally, the function Q^π(s, a) gives the expected return when starting in state **s**, performing action **a** and following π.\n",
    "\n",
    "Again, we can define the optimal Q-function Q∗(s, a) that gives the expected total reward for your agent when starting at s and picks action a. That is, the optimal Q-function tells your agent how good of a choice is picking a when at state s.\n",
    "\n",
    "There is a relationship between the two optimal functions V∗ and Q∗. It is given by:\n",
    "![title](images/v_star.png)\n",
    "\n",
    "That is, the maximum expected total reward when starting at s is the maximum of Q∗(s, a) over all possible actions.\n",
    "\n",
    "Using Q∗(s, a) we can extract the optimal policy π∗ by choosing the action aa that gives maximum reward Q∗(s, a) for state s. We have:\n",
    "![title](images/pi_star.png)\n",
    "\n",
    "\n",
    "## Temporal-Difference (TD)\n",
    "\n",
    "- TD is a combination of ideas from Monto-Carlo methods and DP methods\n",
    "- TD methods learn directly from episodes of raw experience without a model, like MC\n",
    "- TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (i.e. they bootstrap), like DP\n",
    "- TD updates a guess towards a guess\n",
    "- General Update Rule: Q[s,a] += learning_rate * (td_target - Q[s,a]); where as (td_target - Q[s,a]) is called the TD Error\n",
    "- TD Methods: \n",
    "- - Evaluation/Prediction: TD(0), TD(lambda)\n",
    "- - Control: SARSA, Q-Learning\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "- Q-Learning is an Off-Policy, model-free algorithm based on well-known Bellman Equation\n",
    "- Estimate Q* for the greedy behavior policy p:\n",
    "- TD Target for Q-Learning: R[t+1] + discount_factor * max(Q[next_state])\n",
    "\n",
    "**Algorithm**\n",
    "![title](images/q_learning_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import datetime\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "from matplotlib.cbook import MatplotlibDeprecationWarning\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib as mpl\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "from pylab import rc\n",
    "import json\n",
    "import random\n",
    "#from itertools import izip\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run flappy_bird_env_clone.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    The Agent class with utility methods   \n",
    "    \"\"\"\n",
    "    def __init__(self,                \n",
    "                 epsilon=.2, epsilon_decay=True, epsilon_decay_param=.01, \n",
    "                 tau=100, tau_decay=True, tau_decay_param=.01, \n",
    "                 policy_strategy='e-greedy'):\n",
    "        \n",
    "        \"\"\"Initialize agent parameters.\n",
    "        :param epsilon: Float value (0, 1) prob of taking random action vs. taking greedy action.\n",
    "        :param epsilon_decay: Bool indicating whether to use decay of epsilon over episodes.\n",
    "        :param epsilon_decay_param: Float param for decay given by epsilon*e^(-epsilon_decay_param * episode)\n",
    "        :param tau: Float value for temp. param in the softmax, tau -> 0 = greedy, tau -> infinity = random.\n",
    "        :param tau_decay: Bool indicating whether to use decay of tau over episodes.\n",
    "        :param tau_decay_param: Float param for decay given by tau*e^(-tau_decay_param * episode)\n",
    "        :param policy_strategy: String in {softmax, e-greedy, random}, exploration vs exploitation strategy.  \n",
    "        \"\"\"              \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_decay_param = epsilon_decay_param\n",
    "        self.tau = tau\n",
    "        self.tau_decay = tau_decay\n",
    "        self.tau_decay_param = tau_decay_param\n",
    "        self.policy_strategy = policy_strategy  \n",
    "        \n",
    "        self.moves = []\n",
    "        self.name = 'agent'\n",
    "            \n",
    "    #end def __init__  \n",
    "    \n",
    "    def choose_action(self, s):\n",
    "        \"\"\"Choose action for a TD algorithm that is updating using q values.\n",
    "\n",
    "        The policy strategy for choosing an action is either chosen using a\n",
    "        softmax strategy, epsilon greedy strategy, greedy strategy, or a random strategy.\n",
    "\n",
    "        :param s: Integer index of the current state index the agent is in.\n",
    "\n",
    "        :return a: Integer index of the chosen index for the action to take.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.policy_strategy == 'softmax':\n",
    "            a = self.softmax(s)\n",
    "        elif self.policy_strategy == 'e-greedy':\n",
    "            a = self.epsilon_greedy(s)\n",
    "        elif self.policy_strategy == 'greedy':\n",
    "            a = self.random_policy(self.qvalues[s])\n",
    "        else:\n",
    "            a = np.random.choice(self.actions)\n",
    "\n",
    "        return a\n",
    "    \n",
    "    def random_policy(self, list):\n",
    "        \n",
    "        \"\"\"Helper function to get the argmax of an array breaking ties randomly.\n",
    "        \n",
    "        :param arr: 1D or 1D numpy array to find the argmax for.\n",
    "    \n",
    "        :return choice or argmax_array: Choice is integer index of array with \n",
    "        the max value, argmax_array is array of integer index of max value in each \n",
    "        row of the original array.\n",
    "        \"\"\"\n",
    "    \n",
    "        arr = np.array(list)\n",
    "        \n",
    "        if len(arr.shape) == 1:\n",
    "            choice = np.random.choice(np.flatnonzero(arr == arr.max()))\n",
    "            return choice\n",
    "        else:\n",
    "            N = arr.shape[0]\n",
    "            argmax_array = np.zeros(N)\n",
    "    \n",
    "            for i in range(N):\n",
    "                choice = np.random.choice(np.flatnonzero(arr[i] == arr[i].max()))\n",
    "                argmax_array[i] = choice\n",
    "    \n",
    "        argmax_array = argmax_array.astype(int)\n",
    "    \n",
    "        return argmax_array\n",
    "    \n",
    "    #end def random_policy\n",
    "    \n",
    "    def epsilon_greedy(self, s):\n",
    "        \"\"\"Epsilon greedy exploration-exploitation strategy.\n",
    "\n",
    "        This policy strategy selects the current best action with probability\n",
    "        of 1 - epsilon, and a random action with probability epsilon.\n",
    "        \n",
    "        :param s: Integer index of the current state index the agent is in.\n",
    "\n",
    "        :return a: Integer index of the chosen index for the agent to take.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.epsilon_decay:\n",
    "            epsilon = self.epsilon * np.exp(-self.epsilon_decay_param * len(self.moves))\n",
    "        else:\n",
    "            epsilon = self.epsilon      \n",
    "            \n",
    "        if not np.random.binomial(1, epsilon):\n",
    "            a = self.random_policy(self.qvalues[s])\n",
    "        else:\n",
    "            a = np.random.choice(self.actions)\n",
    "\n",
    "        return a\n",
    "    #end def epsilon_greedy\n",
    "    \n",
    "    def softmax(self, s):\n",
    "        \"\"\"Softmax exploration-exploitation strategy.\n",
    "\n",
    "        This policy strategy uses a boltzman distribution with a temperature \n",
    "        parameter tau, to assign the probabilities of choosing an action based\n",
    "        off of the current q value of the state and action.\n",
    "\n",
    "        :param s: Integer index of the current state index the agent is in.\n",
    "\n",
    "        :return a: Integer index of the chosen index for the agent to take.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.tau_decay:\n",
    "            # Capping the minimum value of tau to prevent overflow issues.\n",
    "            tau = max(self.tau * np.exp(-self.tau_decay_param * self.episode), .1)\n",
    "        else:\n",
    "            tau = self.tau\n",
    "\n",
    "        if len(self.tau_choices) <= self.episode:\n",
    "            self.tau_choices.append(tau)\n",
    "\n",
    "        exp = lambda s, a: np.exp(self.qvalues[s, a]/tau)\n",
    "        values = []\n",
    "        probs = []\n",
    "\n",
    "        for a in self.actions:\n",
    "            # Catching overflow and returning random action if it occurs.\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('error')\n",
    "                try:\n",
    "                    value = exp(s, a)\n",
    "                except RuntimeWarning:\n",
    "                    return self.random_policy(self.qvalues[s])\n",
    "\n",
    "            values.append(value) \n",
    "        \n",
    "        total = sum(values)\n",
    "        probs = [val/total for val in values]\n",
    "\n",
    "        try:\n",
    "            sample = np.random.multinomial(1, probs).tolist()\n",
    "            a = sample.index(1)\n",
    "        except:\n",
    "            # Return random action if there is a overflow issue.\n",
    "            a = self.random_policy(self.qvalues[s])\n",
    "\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlappyBirdAgent(Agent):\n",
    "    \"\"\"\n",
    "    The Agent class that applies the Qlearning logic to Flappy bird game\n",
    "    After every iteration (iteration = 1 game that ends with the bird dying) updates Q values\n",
    "    After every SAVE_N iterations, saves the Q values to the local JSON file\n",
    "    \"\"\"\n",
    "    def __init__(self,                \n",
    "                 epsilon=.2, epsilon_decay=True, epsilon_decay_param=.01, \n",
    "                 tau=100, tau_decay=True, tau_decay_param=.01, \n",
    "                 policy_strategy='e-greedy'):\n",
    "        \n",
    "        \"\"\"Initialize agent parameters.\n",
    "        :param epsilon: Float value (0, 1) prob of taking random action vs. taking greedy action.\n",
    "        :param epsilon_decay: Bool indicating whether to use decay of epsilon over episodes.\n",
    "        :param epsilon_decay_param: Float param for decay given by epsilon*e^(-epsilon_decay_param * episode)\n",
    "        :param tau: Float value for temp. param in the softmax, tau -> 0 = greedy, tau -> infinity = random.\n",
    "        :param tau_decay: Bool indicating whether to use decay of tau over episodes.\n",
    "        :param tau_decay_param: Float param for decay given by tau*e^(-tau_decay_param * episode)\n",
    "        :param policy_strategy: String in {softmax, e-greedy, random}, exploration vs exploitation strategy.  \n",
    "        \"\"\"       \n",
    "        \n",
    "        super(FlappyBirdAgent, self).__init__(epsilon, epsilon_decay, epsilon_decay_param, \n",
    "                                                tau, tau_decay, tau_decay_param, \n",
    "                                                policy_strategy)\n",
    "        \n",
    "        self.gameCNT = 0 # Game count of current run, incremented after every death\n",
    "        self.SAVE_N = 25 # Number of iterations to dump Q values to JSON after  \n",
    "        \n",
    "        self.load_qvalues()\n",
    "        \n",
    "    #end def __init__\n",
    "\n",
    "    def load_qvalues(self):\n",
    "        \"\"\"\n",
    "        Load q values from a JSON file\n",
    "        \"\"\"\n",
    "        self.qvalues = {}\n",
    "        try:\n",
    "            fil = open('data/qvalues.json', 'r')\n",
    "        except IOError:\n",
    "            return\n",
    "        self.qvalues = json.load(fil)\n",
    "        fil.close()\n",
    "    #end def load_qvalues\n",
    "    \n",
    "    def map_state(self, xdif, ydif, vel):\n",
    "        \"\"\"\n",
    "        Map the (xdif, ydif, vel) to the respective state, with regards to the grids\n",
    "        The state is a string, \"xdif_ydif_vel\"\n",
    "\n",
    "        X -> [-40,-30...120] U [140, 210 ... 420]\n",
    "        Y -> [-300, -290 ... 160] U [180, 240 ... 420]\n",
    "        \"\"\"\n",
    "        if xdif < 140:\n",
    "            xdif = int(xdif) - (int(xdif) % 10)\n",
    "        else:\n",
    "            xdif = int(xdif) - (int(xdif) % 70)\n",
    "\n",
    "        if ydif < 180:\n",
    "            ydif = int(ydif) - (int(ydif) % 10)\n",
    "        else:\n",
    "            ydif = int(ydif) - (int(ydif) % 60)\n",
    "\n",
    "        return str(int(xdif))+'_'+str(int(ydif))+'_'+str(vel)\n",
    "    #end def map_state\n",
    "    \n",
    "    def act(self, xdif, ydif, vel):\n",
    "        \"\"\"\n",
    "        Chooses the best action with respect to the current state - Chooses 0 (don't flap) to tie-break\n",
    "        \"\"\"\n",
    "        state = self.map_state(xdif, ydif, vel)\n",
    "\n",
    "        self.moves.append( [self.last_state, self.last_action, state] ) # Add the experience to the history\n",
    "\n",
    "        self.last_state = state # Update the last_state with the current state\n",
    "                \n",
    "        self.last_action = self.choose_action(state)\n",
    "        \n",
    "        return self.last_action   \n",
    "       \n",
    "    #end def act    \n",
    "    \n",
    "    \n",
    "    def get_last_state(self):\n",
    "        return self.last_state\n",
    "    #end def get_last_state\n",
    "   \n",
    "\n",
    "    def save_qvalues(self):\n",
    "        \"\"\"\n",
    "        Save the qvalues to the JSON file\n",
    "        \"\"\"\n",
    "        if self.gameCNT % self.SAVE_N == 0:\n",
    "            fil = open('data/qvalues.json', 'w')\n",
    "            json.dump(self.qvalues, fil)\n",
    "            fil.close()\n",
    "            print('Q-values updated on local file.')\n",
    "        #end if\n",
    "    #end def save_qvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FlappyBirdAgentQLearning(FlappyBirdAgent):\n",
    "    \"\"\"\n",
    "    The Model Free Agent class that applies the Model Free algorithms to Flappy bird game\n",
    "    After every iteration (iteration = 1 game that ends with the bird dying) updates Q values\n",
    "    After every SAVE_N iterations, saves the Q values to the local JSON file\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 learning_rate = 0.7,\n",
    "                 discount_factor = 1.0,\n",
    "                 epsilon=.2, epsilon_decay=True, epsilon_decay_param=.01, \n",
    "                 tau=100, tau_decay=True, tau_decay_param=.01, \n",
    "                 policy_strategy='e-greedy'):\n",
    "        \n",
    "        \"\"\"Initialize agent parameters.\n",
    "        :param epsilon: Float value (0, 1) prob of taking random action vs. taking greedy action.\n",
    "        :param epsilon_decay: Bool indicating whether to use decay of epsilon over episodes.\n",
    "        :param epsilon_decay_param: Float param for decay given by epsilon*e^(-epsilon_decay_param * episode)\n",
    "        :param tau: Float value for temp. param in the softmax, tau -> 0 = greedy, tau -> infinity = random.\n",
    "        :param tau_decay: Bool indicating whether to use decay of tau over episodes.\n",
    "        :param tau_decay_param: Float param for decay given by tau*e^(-tau_decay_param * episode)\n",
    "        :param policy_strategy: String in {softmax, e-greedy, random}, exploration vs exploitation strategy.  \n",
    "        \"\"\"       \n",
    "        super(FlappyBirdAgentQLearning, self).__init__(epsilon, epsilon_decay, epsilon_decay_param, \n",
    "                                                tau, tau_decay, tau_decay_param, \n",
    "                                                policy_strategy)\n",
    "                \n",
    "        self.discount = discount_factor\n",
    "        self.r = {0: 1, 1: -1000} # Reward function\n",
    "        self.lr = learning_rate        \n",
    "        self.last_state = \"420_240_0\"\n",
    "        self.last_action = 0\n",
    "       \n",
    "        self.actions = [0,1] # 0=down 1=up\n",
    "        self.name = 'Flappy Bird Agent with Q-Learning'\n",
    "                \n",
    "    #end def __init__\n",
    "    \n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Update qvalues via iterating over experiences\n",
    "        \"\"\"\n",
    "        history = list(reversed(self.moves))\n",
    "\n",
    "        # Flag if the bird died in the top pipe\n",
    "        high_death_flag = True if int(history[0][2].split('_')[1]) > 120 else False\n",
    "\n",
    "        # Q-learning score updates\n",
    "        t = 1\n",
    "        for exp in history:\n",
    "            state = exp[0]\n",
    "            act = exp[1]\n",
    "            res_state = exp[2]\n",
    "\n",
    "            # Select reward\n",
    "            if t == 1 or t == 2:\n",
    "                cur_reward = self.r[1]\n",
    "            elif high_death_flag and act:\n",
    "                cur_reward = self.r[1]\n",
    "                high_death_flag = False\n",
    "            else:\n",
    "                cur_reward = self.r[0]\n",
    "\n",
    "            # Update\n",
    "            self.qvalues[state][act] = (1-self.lr) * (self.qvalues[state][act]) + \\\n",
    "                                        self.lr * ( cur_reward + self.discount * max(self.qvalues[res_state]) )\n",
    "            t += 1\n",
    "        #end for\n",
    "\n",
    "        self.gameCNT += 1  # increase game count\n",
    "        self.save_qvalues()  # save q values (if game count % SAVE_N == 0)\n",
    "        self.moves = []  # clear history after updating strategies\n",
    "        \n",
    "        #lower exploration rate\n",
    "        self.epsilon = max(0, self.epsilon - self.epsilon_decay_param)\n",
    "    #end def learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAEPCAYAAACupWlqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a40371bef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "max_episodes = 10000\n",
    "fbAgentQLearning = FlappyBirdAgentQLearning()\n",
    "fbEnvQ = FlappyBirdEnviornment(fbAgentQLearning)\n",
    "\n",
    "if __name__ == '__main__':        \n",
    "    fbEnvQ.simulate(max_episodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
