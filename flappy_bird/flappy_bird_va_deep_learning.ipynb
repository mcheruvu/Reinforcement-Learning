{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flappy Bird Value Approximator: Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Function Approximation\n",
    "- Function Approximators: Linear Combinations and Neural Network\n",
    "- Increment Methods: Stochastic Gradient Descent Prediction and Control \n",
    "- Batch Methods: Least Squares Prediction and Control, Experience Replay\n",
    "\n",
    "## Types of Function Approximators\n",
    "- There are many function Approximators – supervised ML algorithms: Linear combinations of features, Neural Network, Decision Tree, etc.\n",
    "- RL can get better benefit from differential function Approximators like Linear and Neural Network algorithms\n",
    "- Incremental methods update the weights on each sample while batch does an updated on each epoch (batch).\n",
    "- Stochastic Gradient Descent (SCD) is an incremental and iterative optimization algorithm to find values of parameters (weights) of a function that minimizes  cost function. \n",
    "- Least Squares method is a form of mathematical regression analysis that finds the line of best fit for a dataset, providing a - visual demonstration of the relationship between the data points.\n",
    "\n",
    "## Neural Network Approximating the Q Function\n",
    "\n",
    "![title](images/va_nonlinear.png)\n",
    "![title](images/va_nonlinear_z.png)\n",
    "\n",
    "\n",
    "## Q-Learning with Non-Linear Approximation\n",
    "\n",
    "- Step 1: Start with initial parameter values\n",
    "- Step 2: Take action a according to an explore or exploit policy, transitioning from s to s’\n",
    "- Step 3: Perform TD update for each parameter\n",
    "     ![title](images/va_nonlinear_eq.png)\n",
    "- Step 4: Go to Step 2\n",
    "\n",
    "Typically the space has many local minima and we no longer guarantee convergence, often works well in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Concepts\n",
    "\n",
    "- Perceptron, the first generation neural network, created a simple mathematical model or a function, mimicking neuron – the basic unit of brain\n",
    "- Sigmoid Neuron improved learning by giving some weightage to the input\n",
    "- Neural Network is a directed graph, organized by layers and layers are created by number of interconnected neurons (nodes)\n",
    "- Typical neural network contains three layers: input, hidden and output. If the hidden layers are more than one, then it is called deep neural network\n",
    "- Actual processing happens in hidden layers where each neuron acts as an activation function to process the input (from previous layers)\n",
    "- The performance of neural network is measured using cost or error function and the dependent weight functions\n",
    "- Forward and backward-propagation are two techniques, neural network users repeatedly until all the input variables are adjusted or calibrated to predict accurate output.\n",
    "- During, forward-propagation, information moves in forward direction and passes through all the layers by applying certain weights to the input parameters. Back-propagation method minimizes the error in the weights by applying an algorithm called gradient descent at each iteration step.\n",
    "\n",
    "![title](images/nn.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "- Deep Learning is an advanced neural network with multiple hidden layers that can work with supervised or unsupervised datasets.\n",
    "- Deep Learning vectorizes the input and converts it into output vector space by decomposing complex geometric and polynomial equations into a series of simple transformations. These transformations go through neuron activation functions at each layer parameterized by input weights.\n",
    "- Convolutional Neural Network (CNN) consists of (1) convolutional layers - to identify the features using weights and biases, followed by (2) fully connected layers - where each neuron is connected from all the neurons of previous layers - to provide nonlinearity, sub-sampling or max-pooling, performance and control data overfitting. Examples include: image and voice recognition.\n",
    "- Recursive Neural Network (RNN) is, another type of Deep Learning, that uses same shared feature weights recursively for processing sequential data, emitted by sensors or the way spoken words are processed in NLP, to produce arbitrary size input and output vectors. Long Short Term Memory (LSTM) is an advanced RNN to learn and remember longer sequences by composing series of repeated modules of neural network. \n",
    "\n",
    "![title](images/deep_nn.png)\n",
    "![title](images/cnn.png)\n",
    "![title](images/rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Sharing and Experience Replay\n",
    "\n",
    "- **Weight Sharing**: Convolutional Neural Network shares weights between local regions Recurrent Neural Network shares weights between time-steps\n",
    "\n",
    "\n",
    "- **Experience Replay**: Store experience (S, A, R, Snext) in a replay buffer and sample mini-batches from it to train the network. This de-correlates the data and leads to better data efficiency. In the beginning, the replay buffer is filled with random experience.Better convergence behavior when training a function approximator. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Network (DQN)\n",
    "\n",
    "- Step 1: Take action at according to e-greedy policy\n",
    "- Step 2: Store transition (st, at, rt+1, st+1) in replay memory D\n",
    "- Step 3: Sample random mini-batch of transitions (s, a, r, s’) from D\n",
    "- Step 4: Compute Q-learning targets w.r.t old, fixed parameters w—\n",
    "- Step 5: Optimize MSE (mean squared error) between Q-network and Q-learning targets\n",
    "    ![title](images/dqn.png)\n",
    "- Step 6: Using variant of stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some key aspects of the implementation:\n",
    "\n",
    "Libraries used: Keras with TensorFlow (**GPU version**) and trained for several hours in Azure Windows Environment.\n",
    "\n",
    "To scale the implementation, we pre-process the images by converting color images to grayscale and then crop the images to 80X80 pixels. And then stack 4 frames together so that the flappy bird velocity is inferred properly.\n",
    "\n",
    "- The input to the neural network consists of an 4x80x80 images. \n",
    "- The first hidden layer convolves 32 filters of 8 x 8 with stride 4 and applies ReLU activation function. \n",
    "- The 2nd layer convolves a 64 filters of 4 x 4 with stride 2 and applies ReLU activation function. \n",
    "- The 3rd layer convolves a 64 filters of 3 x 3 with stride 1 and applies ReLU activation function. \n",
    "- The final hidden layer is fully-connected consisted of 512 rectifier units. \n",
    "- The output layer is a fully-connected linear layer with a single output for each valid action.  \n",
    "\n",
    "**Convolution** actually helps computer to learn higher features like edges and shapes. The example below shows how the edges are stand out after a convolution filter is applied.\n",
    "\n",
    "**Keras** makes it very easy to build convolution neural network. However, there are few things to track:\n",
    "\n",
    "- A) It is important to choose a right initialization method. I choose normal distribution with sigma(σ) =0.01. init=lambda shape, name: normal(shape, scale=0.01, name=name)\n",
    "\n",
    "- B) The ordering of the dimension is important, the default setting is 4x80x80 (Theano setting), so if your input is 80x80x4 (Tensorflow setting) then you are in trouble because the dimension is wrong. Alert: If your input dimension is 80x80x4 (Tensorflow setting) you need to set dim_ordering = tf (tf means tensorflow, th means theano)\n",
    "\n",
    "- C) In Keras, subsample=(2,2) means you down sample the image size from (80x80) to (40x40). In ML literature it is often called “stride”\n",
    "\n",
    "- D) We have used an adaptive learning algorithm called ADAM to do the optimization. The learning rate is 1-e6.\n",
    "\n",
    "**Experience Relay:**\n",
    "\n",
    "It was found that approximation of Q-value using non-linear functions like neural network is not very stable. During the game-play all the episode (s,a,r,s′) are stored in replay memory D. When training the network, random mini-batches from the replay memory are used instead of most the recent transition, which will greatly improve the stability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheru\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import gym\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from collections      import deque\n",
    "from keras.models     import Sequential\n",
    "from keras.layers     import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run flappy_bird_env.py #open AI gym clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueApproxDeepNNModel(object):\n",
    "    \n",
    "    def __init__(self, state_size, action_size, algorithm):\n",
    "        self.algorithm          = algorithm\n",
    "        self.learning_rate      = 0.001        \n",
    "        self.weight_backup      = \"flappy_va_{}.h5\".format(algorithm) \n",
    "        \n",
    "        self.state_size         = state_size        \n",
    "        self.action_size        = action_size    \n",
    "        \n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01        \n",
    "        \n",
    "        self.brain              = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        \n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        \n",
    "        #input layer containing 4 80X80 images\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',input_shape=(80,80,4)))  \n",
    "        \n",
    "        #hidden layers \n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        #Output Layer \n",
    "        model.add(Dense(2))\n",
    "   \n",
    "        #set the loss function\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "\n",
    "        #check file exists to load the weights from\n",
    "        if os.path.isfile(self.weight_backup):\n",
    "            model.load_weights(self.weight_backup)\n",
    "            self.exploration_rate = self.exploration_min\n",
    "        return model\n",
    "\n",
    "    def save_model(self):\n",
    "            self.brain.save(self.weight_backup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueApproxAgent(object):\n",
    "    def __init__(self, state_size, action_size, model):        \n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=2000)\n",
    "        self.learning_rate      = 0.001\n",
    "        self.gamma              = 0.95\n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01\n",
    "        self.exploration_decay  = 0.995\n",
    "        self.model              = model        \n",
    "    \n",
    "    def act(self, state):\n",
    "        #*****************************************\n",
    "        #ACT: agent will randomly select its action at first by a certain percentage, \n",
    "        #called ‘exploration rate’ (or ‘epsilon’). \n",
    "        #At the beginning, it is better for the DQN agent to try \n",
    "        #different things before it starts to search for a pattern\n",
    "        #*****************************************\n",
    "        \n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        act_values = self.model.brain.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "   \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        #****************************************************\n",
    "         #One of the most important steps in the learning process is to remember \n",
    "         #what we did in the past and how the reward was bound to that action\n",
    "        #****************************************************\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    \n",
    "    def replay(self, sample_batch_size):\n",
    "        #*****************************************\n",
    "        #REPLAY: Now that we have our past experiences in an array, \n",
    "        #we can train our neural network. \n",
    "        #We cannot afford to go through all our memory, it will take too many ressources. \n",
    "        #Therefore, we will only take a few samples (sample_batch_size and here set as 32) \n",
    "        #and we will just pick them randomly.\n",
    "        #*************************************************************\n",
    "        #not enough data to train; play another episode\n",
    "        if len(self.memory) < sample_batch_size:\n",
    "            return\n",
    "        \n",
    "        sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "        for state, action, reward, next_state, done in sample_batch:\n",
    "            target = reward\n",
    "            \n",
    "            if not done:\n",
    "                #q-learning\n",
    "                target = reward + self.gamma * np.amax(self.model.brain.predict(next_state)[0])\n",
    "                \n",
    "            target_f = self.model.brain.predict(state)\n",
    "            target_f[0][action] = target\n",
    "                                    \n",
    "            #online learning with One Sample and discard this after fitting it to the model\n",
    "            self.model.brain.fit(state, target_f, epochs=1, verbose=0)\n",
    "            \n",
    "        #adjust the exploration based on decay\n",
    "        if self.exploration_rate > self.exploration_min:\n",
    "            self.exploration_rate *= self.exploration_decay   \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueApproxGame:\n",
    "    \n",
    "    def __init__(self, env, agent, max_iterations= 10000):\n",
    "        \n",
    "        self.sample_batch_size = 32 #only few samples\n",
    "        self.episodes          = max_iterations\n",
    "        self.agent             = agent\n",
    "        self.env               = env #Open AI gym    \n",
    "        self.state_size        = self.env.observation_space.shape[0]\n",
    "        self.action_size       = self.env.action_space.n\n",
    "        \n",
    "        self.start             = datetime.datetime.now()      \n",
    "        self.data              = []\n",
    "\n",
    "    def run(self):\n",
    "        \n",
    "        try:\n",
    "            for index_episode in range(self.episodes):\n",
    "                state = self.env.reset()\n",
    "                #x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "                \n",
    "                #Convert the image into grayscale\n",
    "                state = skimage.color.rgb2gray(state, 3)\n",
    "                \n",
    "                #Resize the image to 80X80\n",
    "                state = skimage.transform.resize(state,(80,80))\n",
    "                \n",
    "                #Rescale the intensity of the image\n",
    "                state = skimage.exposure.rescale_intensity(state,out_range=(0,255))\n",
    "\n",
    "                #Standardazing the values\n",
    "                state = state / 255.0\n",
    "                \n",
    "                #Stacking 4 images together\n",
    "                state_t = np.stack((state, state, state, state), axis=2)\n",
    "                \n",
    "                #Reshaping the data for Keras input layer\n",
    "                state_t = state_t.reshape(1, state_t.shape[0], state_t.shape[1], state_t.shape[2])\n",
    "\n",
    "                done = False\n",
    "                index = 0\n",
    "                episode_reward = 0\n",
    "                \n",
    "                while not done:\n",
    "                    self.env.render(close = True)\n",
    "\n",
    "                   #take action\n",
    "                    action = self.agent.act(state_t)                    \n",
    "                    \n",
    "                    next_state_colored, reward, done, _ = self.env.step(action, 3)\n",
    "                    #print(\"next_state_colored\", next_state_colored)\n",
    "                    next_state = skimage.color.rgb2gray(next_state_colored)\n",
    "                    next_state = skimage.transform.resize(next_state,(80,80))\n",
    "                    next_state = skimage.exposure.rescale_intensity(next_state, out_range=(0, 255))\n",
    "\n",
    "\n",
    "                    next_state = next_state / 255.0\n",
    "\n",
    "\n",
    "                    next_state = next_state.reshape(1, next_state.shape[0], next_state.shape[1], 1) #1x80x80x1\n",
    "                    state_t1 = np.append(next_state, state_t[:, :, :, :3], axis=3)\n",
    "                    \n",
    "                    \n",
    "                    #next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    \n",
    "                    #Stores the image transitions in memory\n",
    "                    self.agent.remember(state_t, action, reward, state_t1, done)\n",
    "                    \n",
    "                    # state = new state\n",
    "                    state_t = state_t1\n",
    "                    index += 1                    \n",
    "                #end while\n",
    "                                \n",
    "                self.save_stats(index_episode, episode_reward, self.env.score)\n",
    "                \n",
    "                self.agent.model.save_model()\n",
    "                    \n",
    "                #train after every episode\n",
    "                self.agent.replay(self.sample_batch_size)\n",
    "        finally:\n",
    "            self.agent.model.save_model()\n",
    "\n",
    "    def save_stats(self, episode, reward, score):\n",
    "                \n",
    "        duration = datetime.datetime.now() - self.start \n",
    "        \n",
    "        if (score >= 50):\n",
    "            print(\"Duration: {} Episode {} Score: {}\".format(duration, \n",
    "                                                                episode, \n",
    "                                                                score))\n",
    "        \n",
    "        self.data.append(json.dumps({ \"algorithm\": self.agent.model.algorithm, \n",
    "                    \"duration\":  \"{}\".format(duration), \n",
    "                    \"episode\":   episode, \n",
    "                    \"reward\":    reward, \n",
    "                    \"score\":     score}))\n",
    "        \n",
    "        if (len(self.data) == 500):\n",
    "            file_name = 'data/stats_flappy_bird_{}.json'.format(self.agent.model.algorithm)\n",
    "            \n",
    "            # delete the old file before saving data for this session\n",
    "            if episode == 1 and os.path.exists(file_name): os.remove(file_name)\n",
    "                \n",
    "            # open the file in append mode to add more json data\n",
    "            file = open(file_name, 'a+')  \n",
    "            for item in self.data:\n",
    "                file.write(item)  \n",
    "                file.write(\",\")\n",
    "            #end for\n",
    "            file.close()\n",
    "            \n",
    "            self.data = []\n",
    "        #end if\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    algorithm = \"Deep_Neural_Network\"\n",
    "    max_episodes = 10000\n",
    "    env = FlappyBirdEnv()\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    \n",
    "    model = ValueApproxDeepNNModel(state_size, action_size, algorithm)\n",
    "    agent = ValueApproxAgent(state_size, action_size, model)\n",
    "    \n",
    "    flappy = ValueApproxGame(env, agent, max_episodes)\n",
    "    \n",
    "    flappy.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
